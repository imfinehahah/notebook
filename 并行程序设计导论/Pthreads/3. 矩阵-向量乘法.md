一个 m x n 的矩阵和一个 n 维向量相乘，串行的伪代码为：

```c++
for (int i = 0; i < m; i++) {
    y[i] = 0.0;
    for (int j = 0; j < n; j++) {
        y[i] += a[i][j] * b[j];
    }
}
```

假设 m = n = 6，线程数 t 为3，则将这些计算按下面情况分配到3个线程中：

线程0计算 y[0], y[1]

线程1计算 y[2], y[3]

线程2计算 y[4], y[5]

计算 y[0] 的代码为：

```c++
y[0] = 0.0;
for (int i = 0; i < n; i++) {
    y[0] += a[0][i] * b[i];
}
```

线程 i 需要访问矩阵 a 的第 i 行以及向量 b 的所有元素，说明最低限度下，向量 b 应该是共享的，但这里矩阵 a 和 向量 y 都是共享的，因为如果这两个是全局变量，主函数就可以简单地通过读取标准输入 stdin 来初始化矩阵 a，乘积向量 y 也可以很容易被主线程打印输出



假设 m 和 n 都能被 t 整除，则每个线程能分配到 m / t 行，线程0处理第一部分的 m / t 行，线程1处理第二部分的 m / t 行，以此类推，则我们给每个线程传递一个编号 i，编号 i 处理 i * m / t, (i + 1) * m / t - 1，代码如下：

```c++
void* mat_cal(void* my_rank) {
    long rank = (long)my_rank;
    int d = m / t;
    for (int i = rank * d; i < (rank + 1) * d; i++) {
        y[i] = 0.0;
        for (int j = 0; j < m; j++) {
            y[i] += a[i][j] * b[j];
        }
    }
    return NULL;
}
```



用 MPI 编写一个矩阵 - 向量乘法的程序工作量比较大，因为其数据结构必须是分布式的，即每个 MPI 的进程只能访问自己的局部内存，所以在 MPI 代码中，需要显示地将向量 b 分配到每一个进程的内存中，该例子表明共享内存的并行程序比分布式内存的程序好写，但下面也会讲到共享内存程序复杂的情况

‘’